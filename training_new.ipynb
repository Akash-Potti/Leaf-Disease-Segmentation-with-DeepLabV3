{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49622fb1-3d28-455a-99d6-9dab71657d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91804b-f68e-4747-a4ea-161f5c6e83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels={0:\"Diseased\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae1975-96df-45c5-9fbf-f69572c783d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir, mask_dir, class_labels):\n",
    "    image_paths = {}\n",
    "    mask_paths = {}\n",
    "    \n",
    "    # load data based on class labels \n",
    "    for cls in class_labels:\n",
    "        class_dir = os.path.join(image_dir, class_labels[cls])\n",
    "        mask_dir_cls = os.path.join(mask_dir, class_labels[cls])\n",
    "        \n",
    "        image_files = os.listdir(class_dir)\n",
    "        mask_files = os.listdir(mask_dir_cls)\n",
    "        \n",
    "        image_paths[cls] = [os.path.join(class_dir, f) for f in image_files]\n",
    "        mask_paths[cls] = [os.path.join(mask_dir_cls, f) for f in mask_files]\n",
    "    \n",
    "    #load images with their respective masks\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(image_dir, filename)\n",
    "            mask_filename = filename.split('.')[0] + 'png'\n",
    "            mask_path = os.path.join(mask_dir, mask_filename)\n",
    "            print(mask_filename)\n",
    "            if os.path.exists(mask_path):\n",
    "                if 'other' not in image_paths:\n",
    "                    image_paths['other'] = []\n",
    "                    mask_paths['other'] = []\n",
    "                image_paths['other'].append(image_path)\n",
    "                mask_paths['other'].append(mask_path)\n",
    "    \n",
    "    # Combine all paths\n",
    "    all_image_paths = []\n",
    "    all_mask_paths = []\n",
    "    for cls in image_paths:\n",
    "        all_image_paths.extend(image_paths[cls])\n",
    "        all_mask_paths.extend(mask_paths[cls])\n",
    "    \n",
    "    return all_image_paths, all_mask_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae068e-09d2-4459-8625-9711c94aefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafDataset(data.Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, class_labels, image_transform=None, mask_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.class_labels = class_labels\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        self.image_paths, self.mask_paths = load_data(image_dir, mask_dir, class_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        mask_path = self.mask_paths[index]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        mask = torch.where(mask > 0, torch.tensor(1), torch.tensor(0))\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb13d2-ff07-4753-bea3-986fc8f2f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    #resize to image 128,128\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "mask_transform = transforms.Compose([\n",
    "    #resize to image 128,128\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246893b-1f66-4ca1-bee8-3dfd5c77b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_dataset = LeafDataset(image_dir='Images/', mask_dir='masks/', class_labels=class_labels, image_transform=image_transform, mask_transform=mask_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84831c-674d-4e4f-b705-3786c9ac0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train validate split 80-20\n",
    "total_size = len(leaf_dataset)\n",
    "print(total_size)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482180d-55e9-4933-b257-dcc7b83a50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f9add-ec46-41b9-a527-3d536bcf1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = data.random_split(leaf_dataset, [train_size, test_size])\n",
    "train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9e4a6-2afc-4e6d-8b6b-ba12d84542b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=1\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.classifier[4] = nn.Sequential(\n",
    "    nn.Conv2d(256, num_classes, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c707b79-6020-450e-a67c-2060f631d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4283c6-8686-4a8c-b5e3-cdd398d103a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "early_stop_counter = 0\n",
    "early_stop_patience = 5\n",
    "best_valid_loss = float('inf')\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d605934-5e44-4053-a0a0-f57421969690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays image,true mask and generated mask\n",
    "def visualize_sample(image, mask_true, mask_generated):\n",
    "    with torch.no_grad():  \n",
    "        if image.device.type == 'cuda': \n",
    "            image = image.cpu()\n",
    "            mask_true = mask_true.cpu()\n",
    "            mask_generated = mask_generated.cpu()\n",
    "\n",
    "        image = np.transpose(image.numpy(), (1, 2, 0))  \n",
    "        mask_true = mask_true.squeeze()\n",
    "        mask_generated = mask_generated.squeeze()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(7, 7))  \n",
    "\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(mask_true, cmap='gray')\n",
    "        axes[1].set_title('True Mask')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        axes[2].imshow(mask_generated, cmap='gray')\n",
    "        axes[2].set_title('Generated Mask')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.tight_layout()  \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b680f87-802b-4486-848c-65fb84e52049",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        \n",
    "        masks = masks.float()\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        print(\"training\")\n",
    "        visualize_sample(images[0], masks[0], outputs[0].sigmoid())\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            outputs = model(images)['out']\n",
    "            \n",
    "            masks = masks.float()\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            print(\"validating\")\n",
    "            visualize_sample(images[0], masks[0], outputs[0].sigmoid())\n",
    "\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
